{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b66f2958",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3bf24e7ba2c45b1c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# BLU02 - Exercises Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd525da6",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bebba7f87f4f151b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import hashlib # for grading\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c6b2b87c",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a23783765364606a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1 Read the Programs data (graded)\n",
    "\n",
    "In this first exercise, we aim to create a single dataframe, combining all programs from all seasons.\n",
    "\n",
    "With a caveat though: **we want to include seasons from the year 1950 onwards**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e539f29e",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e16abeb47fc4ea37",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def read_season(folder_path, file_name):\n",
    "    path = os.path.join(folder_path, file_name)\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def read_programs(folder_path):\n",
    "    files = os.listdir(folder_path)\n",
    "    # Create a list with the name of all files containing programs from\n",
    "    # 1950 inclusive and onwards (just the filename, no complete path.)\n",
    "    # files_from_1950: List[str] = ...\n",
    "    ### BEGIN SOLUTION\n",
    "    files_from_1950 = [f for f in files if int(f.split('-')[0]) >= 1950]\n",
    "    ### END SOLUTION \n",
    "    # Create a list with the dataframes\n",
    "    # seasons: List[pd.DataFrame] = ...\n",
    "    ### BEGIN SOLUTION\n",
    "    seasons = [read_season(folder_path, f) for f in files_from_1950 if '.csv' in f]\n",
    "    ### END SOLUTION\n",
    "    # Use pd.concat to create a single dataframe.\n",
    "    # programs: pd.DataFrame = ...\n",
    "    ### BEGIN SOLUTION\n",
    "    programs = pd.concat(seasons, axis=0, ignore_index=True)\n",
    "    ### END SOLUTION\n",
    "    # Drop the column GUID.\n",
    "    # programs = ...\n",
    "    ### BEGIN SOLUTION\n",
    "    programs = programs.drop(columns='GUID')\n",
    "    ### END SOLUTION\n",
    "    ## Remove Duplicated lines.\n",
    "    ### BEGIN SOLUTION\n",
    "    # programs = ...\n",
    "    programs = programs.drop_duplicates()\n",
    "    ### END SOLUTION\n",
    "    # Set the index to be the column ProgramID\n",
    "    ### BEGIN SOLUTION\n",
    "    programs = programs.set_index('ProgramID')\n",
    "    ### END SOLUTION\n",
    "    return programs\n",
    "\n",
    "programs = read_programs(os.path.join('data', 'programs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d5b3ffb",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2656708135a5a5e4",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert programs['Season'].min() == '1950-51'\n",
    "assert programs['Season'].max() == '2016-17'\n",
    "assert programs.index.name == 'ProgramID'\n",
    "assert programs.shape == (7341, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "31920670",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3383047fa18f453e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2 Read the Concerts data (graded)\n",
    "\n",
    "Although we list all transformations step-by-step for the sake of clarity, we expect you to use method chaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0f86c9d",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e228c2d82463c6b4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def make_concerts(file_path): \n",
    "    # Read concerts data and drop the GUID column.\n",
    "    # concerts: pd.DataFrame = ...\n",
    "    ### BEGIN SOLUTION\n",
    "    concerts = pd.read_csv(file_path)\n",
    "    concerts = concerts.drop(columns=['GUID'])\n",
    "    ### END SOLUTION\n",
    "    # Remember to_datetime? We need it here. We need to parse the columns Date and \n",
    "    # Time. Use pd.to_datetime(...).dt.date for the Date. \n",
    "    # then use the same logic to create the column Hour and Minute from Time column.\n",
    "    ### BEGIN SOLUTION\n",
    "    concerts = concerts.assign(\n",
    "        Date=pd.to_datetime(concerts['Date']).dt.date,\n",
    "        Hour=pd.to_datetime(\n",
    "            concerts['Time']).dt.hour,\n",
    "        Minute=pd.to_datetime(\n",
    "            concerts['Time']).dt.minute,\n",
    "    )\n",
    "    ### END SOLUTION\n",
    "    ## Remove Duplicated lines.\n",
    "    ### BEGIN SOLUTION\n",
    "    concerts = concerts.drop_duplicates()\n",
    "    ### END SOLUTION\n",
    "    ## Remove all lines with empty Time column. Then also drop the Time column.\n",
    "    ### BEGIN SOLUTION\n",
    "    concerts = concerts.dropna(subset=[\"Time\"])\n",
    "    concerts = concerts.drop(\"Time\", axis = 1)\n",
    "    ### END SOLUTION    \n",
    "    \n",
    "    return concerts\n",
    "\n",
    "concerts = make_concerts(os.path.join('data','concerts.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52e0b5f0",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3d21007e725ab889",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert concerts.shape == (20812, 8)\n",
    "assert concerts.Date.min() == datetime.date(1842, 12, 7)\n",
    "assert concerts.Date.max() == datetime.date(2017, 7, 7)\n",
    "assert concerts.Date.max() == datetime.date(2017, 7, 7)\n",
    "assert concerts['Hour'].mode().values[0] == 20\n",
    "assert concerts['Minute'].mode().values[0] == 0\n",
    "assert list(concerts.iloc[1537][['Hour', 'Minute']].values) == [20,30]\n",
    "assert list(concerts.iloc[1201][['Hour', 'Minute']].values) == [20,15]\n",
    "assert set(concerts.columns) == set([\n",
    "    'ProgramID', 'ConcertID', 'EventType', 'Location', 'Venue', 'Date', 'Hour', 'Minute'\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04d6c904",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2ae195e6dd100fc5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3 Combine Programs and Concerts data (graded)\n",
    "\n",
    "Let's combine both dataframes into a single dataset, using an inner join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61a56101",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a65f1464b4525a8b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Remember that you want to join on the index of one of the dataframes.\n",
    "# Join only the concerts with valid ProgramIDs\n",
    "# nyp = ...\n",
    "### BEGIN SOLUTION\n",
    "nyp = concerts.join(programs, on='ProgramID', how='inner')\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26483891",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ac9aef3d5251e36c",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert nyp.shape == (12943, 10)\n",
    "assert set(nyp.columns) == set([\n",
    "    'ProgramID', 'ConcertID', 'EventType', 'Location', 'Venue',\n",
    "    'Date', 'Hour', 'Minute', 'Orchestra', 'Season'\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71271ecf",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1fd4cd11d5139889",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 4 Read Works and Soloists data (graded)\n",
    "\n",
    "We will read the two remaining pieces of data. \n",
    "\n",
    "Again, albeit the step-by-step description, we encourage you to use method chaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb533fb6",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-503e208490ff38e0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def read_works(file_path):\n",
    "    # Read the works data.\n",
    "    # works: pd.DataFrame = ...\n",
    "    ### BEGIN SOLUTION\n",
    "    works = pd.read_csv(file_path)\n",
    "    ### END SOLUTION\n",
    "    # Remove the Intervals (attention to the values in the isInterval column).\n",
    "    # works: pd.DataFrame = ...\n",
    "    ### BEGIN SOLUTION\n",
    "    works = works[~works.isInterval]\n",
    "    ### END SOLUTION\n",
    "    # Select the columns ProgramID, WorkID, ComposerName, WorkTitle, Movement and ConductorName.\n",
    "    ### BEGIN SOLUTION\n",
    "    columns = [\n",
    "        'ProgramID','WorkID', \n",
    "        'ComposerName', 'WorkTitle', 'Movement', \n",
    "        'ConductorName'\n",
    "    ]\n",
    "    works = works.loc[:, columns]\n",
    "    ### END SOLUTION\n",
    "    ## Remove Duplicated lines.\n",
    "    ### BEGIN SOLUTION\n",
    "    # works: pd.DataFrame = ...\n",
    "    works = works.drop_duplicates()\n",
    "    ### END SOLUTION\n",
    "    ## Remove all lines with empty Movement column.\n",
    "    ### BEGIN SOLUTION\n",
    "    # works: pd.DataFrame = ...\n",
    "    works = works.dropna(subset=[\"Movement\"])\n",
    "    ### END SOLUTION    \n",
    "    \n",
    "    return works\n",
    "\n",
    "\n",
    "def read_soloists(file_path):\n",
    "    # Read the soloists data and drop GUID and MovementID Columns\n",
    "    ### BEGIN SOLUTION\n",
    "    soloists = pd.read_csv(file_path)\n",
    "    soloists = soloists.drop(columns=['GUID', 'MovementID'])\n",
    "    ### END SOLUTION\n",
    "    ## Remove Duplicated lines.\n",
    "    ### BEGIN SOLUTION\n",
    "    # soloists: pd.DataFrame = ...\n",
    "    soloists = soloists.drop_duplicates()\n",
    "    ### END SOLUTION\n",
    "    return soloists\n",
    "\n",
    "\n",
    "works = read_works('data/works.csv')\n",
    "soloists = read_soloists('data/soloists.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11f60f07",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b8389314995f18ea",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert works.shape == (24320, 6)\n",
    "assert set(works.columns) == set([\n",
    "    'ProgramID', 'WorkID', 'ComposerName', 'WorkTitle', 'Movement', 'ConductorName'\n",
    "])\n",
    "\n",
    "assert soloists.shape == (50292, 5)\n",
    "assert set(soloists.columns) == set([\n",
    "   'ProgramID', 'WorkID', 'SoloistName', 'SoloistInstrument', 'SoloistRole'\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "839081fe",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c16e4e26e68cd019",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 5 Combine Works and Soloists (graded)\n",
    "\n",
    "Like we did for Programs and Concerts, now we combine Works and Soloists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfae5297",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-08d9a086cc5646cf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Combine both dataframes, again using an inner type of join. An work is identified by the pair\n",
    "# ProgramId, WorkID\n",
    "# works_and_soloists : pd.DataFrame = ....\n",
    "### BEGIN SOLUTION\n",
    "works_and_soloists = pd.merge(works, soloists, on=['WorkID', 'ProgramID'])\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7184d4cd",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4d9f103dfffd311b",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert works_and_soloists.shape == (23578, 9)\n",
    "assert set(works_and_soloists.columns) == set(\n",
    "    [\n",
    "        'ProgramID', 'WorkID', 'ComposerName', 'WorkTitle', 'Movement',\n",
    "        'ConductorName', 'SoloistName', 'SoloistInstrument', 'SoloistRole'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6cddd282",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ab79800d6e447f1e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 6 Combine everything (graded)\n",
    "\n",
    "The final goal here is to create a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a4eee72",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ce1f05022e8cd63a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Combine works_and_soloists and nyp into a single dataframe.\n",
    "# You need to figure out the common column shared between the two dataframes\n",
    "# nyp_merged = ...\n",
    "### BEGIN SOLUTION\n",
    "nyp_merged = pd.merge(nyp, works_and_soloists, on=['ProgramID'])\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55c7dc60",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ce29fa3aec1c244e",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert nyp_merged.shape == (27725, 18)\n",
    "assert set(nyp_merged.columns) == set(\n",
    "    [\n",
    "       'ProgramID', 'ConcertID', 'EventType', 'Location', 'Venue', 'Date',\n",
    "       'Hour', 'Minute', 'Orchestra', 'Season', 'WorkID', 'ComposerName', 'WorkTitle',\n",
    "       'Movement', 'ConductorName', 'SoloistName', 'SoloistInstrument',\n",
    "       'SoloistRole'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c3facfa",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-57f2ca220627e218",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Exercises complete, congratulations! You are about to become a certified data wrangler."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
